from typing import Annotated, Literal, TypedDict

from langchain.chat_models import init_chat_model
from langchain_core.messages import AIMessage, AnyMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_tavily import TavilySearch
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.graph import END, StateGraph
from langgraph.graph.message import add_messages
from langgraph.prebuilt import ToolNode
from utils.load_env import get_openai_api_key, get_tavily_api_key


# -----------------------------
# 1ï¸âƒ£ å®šç¾©ç‹€æ…‹
# -----------------------------
class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    user_info: dict  # å„²å­˜ä½¿ç”¨è€…è³‡è¨Šï¼ˆå§“åã€åœ°é»ç­‰ï¼‰


# -----------------------------
# 2ï¸âƒ£ åˆå§‹åŒ–å·¥å…·
# -----------------------------
api_key = get_openai_api_key()
tavily_api_key = get_tavily_api_key()

search_tool = TavilySearch(api_key=tavily_api_key, max_results=3)
checkpointer = InMemorySaver()


@tool
def web_search(query: str) -> str:
    """Search the web for current information."""
    try:
        results = search_tool.run(query)
        return f"Search results: {results}"
    except Exception as e:
        return f"Search failed: {str(e)}"


tools = [web_search]

# -----------------------------
# 3ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹
# -----------------------------
model = init_chat_model(
    model="gpt-4o-mini", model_provider="openai", api_key=api_key, temperature=0
).bind_tools(tools)


# -----------------------------
# 4ï¸âƒ£ å®šç¾©ç¯€é»å‡½æ•¸
# -----------------------------
def agent_node(state: AgentState) -> AgentState:
    """ä¸»è¦çš„ agent æ¨ç†ç¯€é»"""
    print("ğŸ¤– AGENT THINKING...")

    # ç³»çµ±è¨Šæ¯
    system_msg = SystemMessage(
        content="""
    You are a helpful assistant. Remember user information from the conversation.
    When you need current information, use the web_search tool.
    Think step by step and show your reasoning clearly.
    
    Always check if you have the information needed before using tools.
    """
    )

    # å»ºæ§‹è¨Šæ¯åˆ—è¡¨
    messages = [system_msg] + state["messages"]

    # èª¿ç”¨æ¨¡å‹
    response = model.invoke(messages)

    print(f"ğŸ’­ AGENT RESPONSE: {response.content}")
    if response.tool_calls:
        print(f"ğŸ› ï¸  TOOLS TO CALL: {[tc['name'] for tc in response.tool_calls]}")

    return {"messages": [response]}


def extract_user_info(state: AgentState) -> AgentState:
    """æå–ä¸¦è¨˜æ†¶ä½¿ç”¨è€…è³‡è¨Š"""
    user_info = state.get("user_info", {})

    # ç°¡å–®çš„è³‡è¨Šæå–é‚è¼¯
    for message in state["messages"]:
        if isinstance(message, HumanMessage):
            content = message.content.lower()
            if "my name is" in content or "i'm" in content:
                # æå–å§“å
                words = content.split()
                for i, word in enumerate(words):
                    if word in ["i'm", "name", "is"] and i + 1 < len(words):
                        name = words[i + 1].strip(",.!")
                        if name.isalpha():
                            user_info["name"] = name

            if "live in" in content or "from" in content:
                # æå–åœ°é»
                if "live in" in content:
                    location = content.split("live in")[-1].strip(",.!")
                elif "from" in content:
                    location = content.split("from")[-1].strip(",.!")
                user_info["location"] = location.strip()

    print(f"ğŸ“ UPDATED USER INFO: {user_info}")
    return {"user_info": user_info}


def should_continue(state: AgentState) -> Literal["tools", "end"]:
    """æ±ºå®šæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·"""
    last_message = state["messages"][-1]
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        print("â¡ï¸  CONTINUING TO TOOLS")
        return "tools"
    print("âœ… ENDING CONVERSATION")
    return "end"


# -----------------------------
# 5ï¸âƒ£ å»ºç«‹åœ–
# -----------------------------
def create_agent_graph():
    workflow = StateGraph(AgentState)

    # æ·»åŠ ç¯€é»
    workflow.add_node("extract_info", extract_user_info)
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", ToolNode(tools))

    # å®šç¾©é‚Š
    workflow.set_entry_point("extract_info")
    workflow.add_edge("extract_info", "agent")
    workflow.add_conditional_edges("agent", should_continue, {"tools": "tools", "end": END})
    workflow.add_edge("tools", "agent")

    return workflow.compile(checkpointer=checkpointer)


# -----------------------------
# 6ï¸âƒ£ åŸ·è¡Œå°è©±
# -----------------------------
def run_conversation():
    app = create_agent_graph()

    # åˆå§‹ç‹€æ…‹
    initial_state = {"messages": [], "user_info": {}}

    inputs = [
        "Hi, I'm Bob and I live in SF.",
        "What's my name and where do I live?",
        "What's the current weather in the place I am living?",
    ]

    current_state = initial_state

    for i, user_input in enumerate(inputs, 1):
        print(f"\n{'=' * 70}")
        print(f"CONVERSATION ROUND {i}")
        print(f"{'=' * 70}")
        print(f"ğŸ‘¤ USER: {user_input}")
        print("-" * 50)

        # æ·»åŠ ä½¿ç”¨è€…è¨Šæ¯
        current_state["messages"].append(HumanMessage(content=user_input))

        # åŸ·è¡Œåœ–ä¸¦é¡¯ç¤ºæ¯å€‹æ­¥é©Ÿ
        step_count = 0
        for step in app.stream(
            current_state, config={"configurable": {"thread_id": "1"}}, stream_mode="updates"
        ):
            step_count += 1
            print(f"\nğŸ“‹ STEP {step_count}:")

            for node_name, node_output in step.items():
                print(f"  ğŸ”¹ NODE: {node_name}")

                # æ›´æ–°ç‹€æ…‹
                current_state.update(node_output)

                # é¡¯ç¤ºæ–°è¨Šæ¯
                if "messages" in node_output:
                    for msg in node_output["messages"]:
                        if isinstance(msg, AIMessage):
                            print(f"    ğŸ¤– AI: {msg.content}")
                            if hasattr(msg, "tool_calls") and msg.tool_calls:
                                for tc in msg.tool_calls:
                                    print(f"    ğŸ”§ TOOL CALL: {tc['name']} with {tc['args']}")
                        elif isinstance(msg, ToolMessage):
                            print(f"    ğŸ› ï¸  TOOL RESULT: {msg.content[:200]}...")

                if "user_info" in node_output:
                    print(f"    ğŸ“ USER INFO: {node_output['user_info']}")

        print("\nğŸ’¾ FINAL STATE:")
        print(f"   Messages: {len(current_state['messages'])}")
        print(f"   User Info: {current_state['user_info']}")
        print(f"\n{'=' * 70}")


# -----------------------------
# 7ï¸âƒ£ å¦ä¸€å€‹ç‰ˆæœ¬ï¼šæ›´è©³ç´°çš„ streaming
# -----------------------------
def run_conversation_detailed_stream():
    """ä½¿ç”¨æ›´è©³ç´°çš„ streaming æ¨¡å¼"""
    app = create_agent_graph()

    initial_state = {"messages": [], "user_info": {}}

    inputs = [
        "Hi, I'm Bob and I live in SF.",
        "What's my name and where do I live?",
        "What's the current weather in the place I am living?",
    ]

    current_state = initial_state

    for user_input in inputs:
        print(f"\nğŸ‘¤ USER: {user_input}")
        print("=" * 60)

        current_state["messages"].append(HumanMessage(content=user_input))

        # ä½¿ç”¨ debug stream æ¨¡å¼
        for chunk in app.stream(
            current_state,
            stream_mode="debug",  # é¡¯ç¤ºæ›´å¤šç´°ç¯€
            debug=True,
        ):
            print(f"ğŸ“¦ CHUNK: {chunk}")

        print("=" * 60)


if __name__ == "__main__":
    print("ğŸš€ Starting LangGraph Agent...")
    run_conversation()
